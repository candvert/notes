**作者:**
* Ashish Vaswani (Google Brain)
* Noam Shazeer (Google Brain)
* Niki Parmar (Google Research)
* Jakob Uszkoreit (Google Research)
* Llion Jones (Google Research)
* Aidan N. Gomez (多伦多大学)
* Łukasz Kaiser (Google Brain)
* Illia Polosukhin

(* 贡献相同。排名顺序随机。在谷歌大脑工作期间完成。)

### 摘要

主流的序列转导模型基于复杂的循环神经网络（RNN）或卷积神经网络（CNN），并采用编码器-解码器（encoder-decoder）配置。性能最佳的模型还通过注意力机制连接编码器和解码器。

我们提出了一种新的、简单的网络架构——**Transformer**，它完全基于注意力机制，彻底摒弃了循环和卷积。

在两项机器翻译任务上的实验表明，这些模型在质量上更优，同时具有更高的并行性，并且训练所需时间显著减少。我们的模型在 WMT 2014 英德翻译任务上实现了 28.4 的 BLEU 值，比现有的最佳结果（包括集成模型）高出 2.0 BLEU 以上。在 WMT 2014 英法翻译任务上，我们的模型在 8 个 GPU 上训练了 3.5 天后，创下了 41.0 的 BLEU 分数，刷新了单模型的最新记录，而训练成本仅为文献中最佳模型的一小部分。

我们通过将其成功应用于训练数据量或大或小的英语成分句法分析任务，表明 Transformer 可以很好地推广到其他任务。

---

### 1. 引言

循环神经网络（RNN），特别是长短期记忆（LSTM）[12] 和门控循环 [7] 神经网络，已经在序列建模和转导问题（如语言建模和机器翻译 [31, 2, 5]）中牢固确立了其最先进方法的地位。

循环模型通常沿着输入和输出序列的符号位置进行计算。它们生成一系列隐藏状态 $h_{t}$，作为前一个隐藏状态 $h_{t-1}$ 和当前位置 t 的输入的函数。这种固有的顺序性阻碍了训练样本内的并行化，这在序列较长时变得至关重要，因为内存限制了跨样本的批处理。

注意力机制已成为各种任务中序列建模和转导模型的重要组成部分，它允许模型对依赖关系进行建模，而不必考虑它们在输入或输出序列中的距离 [2, 17]。然而，在除少数情况外 [25]，这种注意力机制都是与循环网络结合使用的。

在这项工作中，我们提出了 **Transformer**，这是一种摒弃了循环的模型架构，转而完全依赖注意力机制来绘制输入和输出之间的全局依赖关系。Transformer 允许显著提高并行化程度，并且在 8 个 P100 GPU 上仅训练 12 小时后，就能在翻译质量上达到新的SOTA（state of the art）。

### 2. 背景

减少顺序计算的目标也是 Extended Neural GPU [21]、ByteNet [16] 和 ConvS2S [9] 的基础，它们都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，关联两个任意输入或输出位置信号所需的操作数量随位置间距离的增长而增长，ConvS2S 是线性增长，ByteNet 是对数增长。这使得学习远距离位置之间的依赖关系变得更加困难 [11]。

在 Transformer 中，这被减少为恒定数量的操作，尽管代价是由于平均注意力加权位置而导致有效分辨率降低，我们使用 3.2 节中描述的多头注意力（Multi-Head Attention）来抵消这种影响。

自注意力（Self-attention），有时也称为内部注意力（intra-attention），是一种将单个序列的不同位置关联起来以计算序列表示的注意力机制。自注意力已成功用于多种任务，包括阅读理解、摘要、文本蕴含和学习任务无关的句子表示 [4, 25, 26, 20]。

然而，据我们所知，Transformer 是第一个完全依赖自注意力来计算其输入和输出表示，而不使用 RNN 或卷积的转导模型。

### 3. 模型架构

大多数具有竞争力的神经序列转导模型都具有编码器-解码器结构 [5, 2, 31]。编码器将符号表示的输入序列 $(x_{1},...,x_{n})$ 映射到连续表示的序列 $z=(z_{1},...,z_{n})$。给定 $z$，解码器然后生成一个符号的输出序列 $(y_{1},...,y_{m})$，一次生成一个元素。在每一步，模型都是自回归的（auto-regressive）[10]，在生成下一个符号时，会使用先前生成的符号作为额外输入。

Transformer 遵循这种整体架构，编码器和解码器都使用堆叠的自注意力和逐点（point-wise）、全连接层，分别如图 1 的左半部分和右半部分所示。

#### 3.1 编码器和解码器堆叠

* **编码器：** 编码器由 $N=6$ 个相同的层堆叠而成。每层有两个子层。第一个是多头自注意力机制，第二个是简单的、逐位置（position-wise）的全连接前馈网络。我们在每个子层周围使用残差连接 [10]，然后进行层归一化 [1]。也就是说，每个子层的输出是 $LayerNorm(x+Sublayer(x))$。为了方便这些残差连接，模型中的所有子层以及嵌入层都产生维度为 $d_{model}=512$ 的输出。

* **解码器：** 解码器同样由 $N=6$ 个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器堆叠的输出执行多头注意力。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器堆叠中的自注意力子层，以防止位置关注到后续位置。这种掩码（masking），再加上输出嵌入偏移一个位置的事实，确保了对位置 i 的预测只能依赖于位置小于 i 的已知输出。

#### 3.2 注意力

注意力函数可以描述为将一个查询（query）和一组键（key）-值（value）对映射到一个输出，其中查询、键、值和输出都是向量。输出是值的加权总和，其中分配给每个值的权重是通过查询与相应键的兼容性函数来计算的。

**3.2.1 缩放点积注意力 (Scaled Dot-Product Attention)**

我们称我们的特定注意力为“缩放点积注意力”（图 2）。输入由维度为 $d_{k}$ 的查询和键，以及维度为 $d_{v}$ 的值组成。我们计算查询与所有键的点积，然后将每个点积除以 $\sqrt{d_{k}}$，并应用 softmax 函数来获得值的权重。

在实践中，我们同时计算一组查询的注意力函数，并将它们打包成一个矩阵 $Q$。键和值也分别打包成矩阵 $K$ 和 $V$。我们计算输出矩阵如下：

$$Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$$

两种最常用的注意力函数是加法注意力（additive attention）[2] 和点积（乘法）注意力。点积注意力与我们的算法相同，只是没有 $\sqrt{d_{k}}$ 的缩放因子。我们怀疑，对于较大的 $d_{k}$ 值，点积的幅度会增长得过大，导致 softmax 函数进入梯度极小的区域。为了抵消这一点，我们使用 $1/\sqrt{d_{k}}$ 来缩放点积。

**3.2.2 多头注意力 (Multi-Head Attention)**

我们发现，将查询、键和值分别使用 $h$ 次不同的、可学习的线性投影，将其投影到 $d_{k}$、$d_{k}$ 和 $d_{v}$ 维度，而不是执行单个 $d_{model}$ 维的注意力函数，这样做更有益。然后，我们在这些投影后的查询、键和值上并行执行注意力函数，产生 $d_{v}$ 维的输出值。将这些值连接起来，就得到了最终的值（如图 2 所示）。

多头注意力允许模型在不同位置共同关注来自不同表示子空间的信息。

$$JultiHead(Q,K,V)=Concat(head_{1},...,head_{h}) \quad \text{其中 } head_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$$

在这项工作中，我们使用 $h=8$ 个并行的注意力层（或头）。对于每个头，我们使用 $d_{k}=d_{v}=d_{model}/h=64$。

**3.2.3 注意力在模型中的应用**

Transformer 以三种不同方式使用多头注意力：

1.  在“编码器-解码器注意力”层中，查询来自前一个解码器层，而键和值来自编码器的输出。这允许解码器中的每个位置都能关注到输入序列中的所有位置。
2.  编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，即编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层的所有位置。
3.  类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的信息向左流动（leftward information flow），以保持自回归属性。

#### 3.3 逐位置前馈网络

除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个全连接前馈网络，该网络分别且相同地应用于每个位置。它由两个线性变换组成，中间有一个 ReLU 激活函数。

$$FFN(x)=max(0,xW_{1}+b_{1})W_{2}+b_{2}$$

输入和输出的维度是 $d_{model}=512$，内部层的维度是 2048。

#### 3.4 嵌入和 Softmax

与其他序列转导模型类似，我们使用学习到的嵌入（embedding）将输入词元（token）和输出词元转换为 $d_{model}$ 维的向量。我们还使用通常的学习型线性变换和 softmax 函数将解码器输出转换为预测的下一个词元的概率。在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享同一个权重矩阵。

#### 3.5 位置编码 (Positional Encoding)

由于我们的模型不包含循环和卷积，为了让模型利用序列的顺序，我们必须注入一些关于序列中词元相对或绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器堆叠底部的输入嵌入中。位置编码具有与嵌入相同的维度 $d_{model}$，因此两者可以相加。

在这项工作中，我们使用不同频率的正弦和余弦函数：

$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$
$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$

其中 $pos$ 是位置，$i$ 是维度。我们选择这个函数是因为我们假设它能让模型轻松学会通过相对位置来“关注”，因为对于任何固定的偏移量 k，$PE_{pos+k}$ 都可以表示为 $PE_{pos}$ 的线性函数。

---

### 4. 为什么选择自注意力

在本节中，我们将自注意力层与循环层和卷积层进行比较，这三种层通常用于将一个可变长度的符号表示序列 $(x_{1},...,x_{n})$ 映射到另一个等长的序列 $(z_{1},...,z_{n})$。

为了说明我们使用自注意力的动机，我们考虑三个期望的特性：

1.  **每层的总计算复杂度。**
2.  **可并行的计算量**，以所需的最小顺序操作数来衡量。
3.  **网络中远距离依赖关系之间的路径长度。** 学习远距离依赖是许多序列转导任务中的关键挑战。路径越短，学习远距离依赖就越容易 [11]。

**表 1：不同层类型的最大路径长度、每层复杂度和最小顺序操作数。**
n 是序列长度，d 是表示维度，k 是卷积核大小，r 是受限自注意力中的邻域大小。

| 层类型                       | 每层复杂度                    | 顺序操作数  | 最大路径长度          |
| :------------------------ | :----------------------- | :----- | :-------------- |
| **自注意力 (Self-Attention)** | $O(n^{2}\cdot d)$        | $O(1)$ | $O(1)$          |
| 循环 (Recurrent)            | $O(n\cdot d^{2})$        | $O(n)$ | $O(n)$          |
| 卷积 (Convolutional)        | $O(k\cdot n\cdot d^{2})$ | $O(1)$ | $O(log_{k}(n))$ |
| 自注意力 (受限)                 | $O(r\cdot n\cdot d)$     | $O(1)$ | $O(n/r)$        |

如表 1 所示，自注意力层以恒定数量的顺序执行操作连接所有位置，而循环层需要 $O(n)$ 的顺序操作。

在计算复杂度方面，当序列长度 n 小于表示维度 d 时（这在SOTA的机器翻译模型中通常是这种情况），自注意力层快于循环层。

单个核宽度 $k<n$ 的卷积层不会连接所有输入和输出位置对。要做到这一点，需要一个由 $O(n/k)$ 个卷积层组成的堆叠（对于连续核），或 $O(log_{k}(n))$（对于空洞卷积 [16]），这增加了网络中任意两个位置之间的最长路径长度。

作为一个附带好处，自注意力可以产生更具可解释性的模型。我们检查了模型的注意力分布，并在附录中展示和讨论了示例。

---

### 5. 训练

#### 5.1 训练数据和批处理

我们在 WMT 2014 英德标准数据集上进行了训练，该数据集包含约 450 万个句子对。句子使用字节对编码（byte-pair encoding）[3] 进行编码，共享的源-目标词汇表大小约为 37000 个词元。对于英法翻译，我们使用了更大的 WMT 2014 英法数据集，包含 3600 万个句子，并将词元拆分为 32000 个 word-piece 词汇表 [34]。

#### 5.2 硬件和时间表

我们在 1 台配备 8 个 NVIDIA P100 GPU 的机器上训练我们的模型。对于我们的基础模型，每个训练步骤耗时约 0.4 秒。我们总共训练了 100,000 步或 12 小时。对于我们的大型模型，步长时间为 1.0 秒，模型训练了 300,000 步或 3.5 天。

#### 5.3 优化器

我们使用了 Adam 优化器 [18]，$\beta_{1}=0.9$，$\beta_{2}=0.98$，$\epsilon=10^{-9}$。我们根据以下公式在训练过程中改变学习率：

$trate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$

我们使用了 $warmup\_steps = 4000$。

#### 5.4 正则化

我们在训练期间采用了三种正则化方法：

1.  **残差 Dropout** [30]：我们对每个子层的输出应用 dropout，在将其添加到子层输入并进行归一化之前。对于基础模型，我们使用 $P_{drop}=0.1$ 的比率。
2.  **注意力 Dropout**：我们在 softmax 激活之后和乘以 $V$ 之前，对注意力权重应用 dropout。
3.  **标签平滑 (Label Smoothing)** [32]：我们在训练期间使用了 $\epsilon_{ls}=0.1$ 的标签平滑。这降低了模型的困惑度（perplexity），但提高了准确率和 BLEU 分数。

---

### 6. 结果

#### 6.1 机器翻译

**表 2：在英德和英法 newstest2014 测试中，Transformer 取得了比以往SOTA模型更好的 BLEU 分数，而训练成本仅为一小部分。**

| 模型                           | BLEU (EN-DE) | BLEU (EN-FR) | 训练成本 (FLOPs)          |
| :--------------------------- | :----------- | :----------- | :-------------------- |
| ByteNet [16]                 | 23.75        |              | $2.3\cdot10^{19}$     |
| GNMT + RL [34]               | 24.6         | 39.92        | $2.3\cdot10^{19}$     |
| ConvS2S [9]                  | 25.16        | 40.46        | $9.6\cdot10^{18}$     |
| ... (其他模型)                   | ...          | ...          | ...                   |
| **Transformer (base model)** | **27.3**     | **38.1**     | **$3.3\cdot10^{18}$** |
| **Transformer (big)**        | **28.4**     | **41.0**     | **$2.3\cdot10^{19}$** |

* **英德翻译：** 在 WMT 2014 英德翻译任务上，我们的大型 Transformer 模型（Transformer (big)）的表现超过了之前报道的最佳模型（包括集成模型）2.0 BLEU 以上，创下了 28.4 的SOTA BLEU 分数。训练耗时 3.5 天（在 8 个 P100 GPU 上）。
* **英法翻译：** 在 WMT 2014 英法翻译任务上，我们的大型模型取得了 41.0 的 BLEU 分数，超过了所有先前发布的单模型，而训练成本不到之前SOTA模型的 1/4。

#### 6.2 模型变体

**表 3：Transformer 架构的变体。** (在英德翻译开发集 newstest2013 上的表现)

| 变体 (描述) | BLEU (dev) | 参数量 (百万) |
| :--- | :--- | :--- |
| **base (N=6, $d_{model}$=512, h=8)** | **25.8** | **65** |
| (A) h=1 (单头注意力) | 24.9 | 65 |
| (A) h=4 | 25.5 | 65 |
| (A) h=16 | 25.8 | 65 |
| (B) $d_{k}=16$ (减小键维度) | 25.1 | 58 |
| (C) N=2 (减少层数) | 23.7 | 36 |
| (C) $d_{model}$=256 (减小模型维度) | 24.5 | 28 |
| (D) $P_{drop}=0.0$ (无 dropout) | 24.6 | 65 |
| **big (N=6, $d_{model}$=1024, h=16)** | **26.4** | **213** |

* (A) 行显示，虽然单头注意力比最佳设置差 0.9 BLEU，但过多的头（h=32）也会导致质量下降。
* (B) 行观察到，减小键的大小 $d_{k}$ 会损害模型质量。
* (C) 和 (D) 行显示，正如预期的那样，更大的模型更好，并且 dropout 对于避免过拟合非常有帮助。

#### 6.3 英语成分句法分析

为了评估 Transformer 是否可以推广到其他任务，我们进行了英语成分句法分析的实验。

**表 4：Transformer 很好地推广到英语成分句法分析。** (WSJ 23 F1 分数)

| 解析器 | 训练数据 | WSJ 23 F1 |
| :--- | :--- | :--- |
| Vinyals & Kaiser (2014) [33] | 仅 WSJ | 88.3 |
| Petrov et al. (2006) [27] | 仅 WSJ | 90.4 |
| Dyer et al. (2016) [8] | 仅 WSJ | 91.7 |
| **Transformer (4 layers)** | **仅 WSJ** | **91.3** |
| McClosky et al. (2006) [24] | 半监督 | 92.1 |
| Vinyals & Kaiser (2014) [33] | 半监督 | 92.1 |
| **Transformer (4 layers)** | **半监督** | **92.7** |

表 4 中的结果表明，尽管缺乏针对特定任务的调优，我们的模型表现得出奇地好，其结果优于除 Recurrent Neural Network Grammar [8] 之外的所有先前报道的模型。与 RNN 序列到序列模型 [33] 相比，Transformer 即使仅在 40K 句子的 WSJ 训练集上训练，其表现也优于 BerkeleyParser [27]。

---

### 7. 结论

在这项工作中，我们提出了 Transformer，这是第一个完全基于注意力的序列转导模型，用多头自注意力取代了编码器-解码器架构中最常用的循环层。

对于翻译任务，Transformer 的训练速度明显快于基于循环层或卷积层的架构。在 WMT 2014 英德和 WMT 2014 英法翻译任务上，我们都取得了SOTA的成绩。

我们对基于注意力的模型的未来感到兴奋，并计划将它们应用于其他任务。我们计划将 Transformer 扩展到涉及文本以外的输入和输出模态的问题，并研究局部的、受限的注意力机制，以有效处理大型输入和输出，如图像、音频和视频。使生成过程的顺序性降低是我们的另一个研究目标。

---
*(翻译结束)*